# -*- coding: utf-8 -*-
"""Eksperimen_MSML (Healthcare)_Rahma_Putri.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tj8GfLgbosTSxyKf8WYdms-HA9iXWoK0

# **1. Perkenalan Dataset**

Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:

1. **Sumber Dataset**:  
   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

import pandas as pd  # Untuk manipulasi data
import numpy as np  # Untuk operasi numerik
import matplotlib.pyplot as plt  # Untuk visualisasi
import seaborn as sns  # Untuk visualisasi yang lebih menarik
from sklearn.preprocessing import StandardScaler  # Untuk preprocessing
from sklearn.preprocessing import LabelEncoder  # Encoding Data Kategorikal
from sklearn.model_selection import train_test_split # Untuk membagi data

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

# Muat dataset
# Ganti dengan path file yang sesuai
file_path = 'healthcare_dataset.csv'
df = pd.read_csv(file_path)

# Tampilkan beberapa baris pertama dataset
print("5 Baris Pertama Dataset:")
print(df.head())

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.
"""

# Informasi umum: tipe data, jumlah non-null
print("\nInformasi Umum Dataset:")
df.info()

# Ukuran dataset (jumlah baris dan kolom)
print(f"\nUkuran Dataset: {df.shape}")

# Nama kolom
print("\nNama-nama Kolom:")
print(df.columns)

# Statistik deskriptif kolom numerik
print("\nStatistik Deskriptif (Numerik):")
print(df.describe())

# Nilai unik pada kolom kategorikal
print("\nJumlah Nilai Unik Kolom Kategorikal:")
print(df.select_dtypes(include='object').nunique())

# Distribusi usia pasien (Age)
plt.figure(figsize=(8, 5))
sns.histplot(df['Age'], kde=True)
plt.title('Distribusi Usia Pasien')
plt.xlabel('Usia')
plt.ylabel('Frekuensi')
plt.show()

# Distribusi berdasarkan Medical Condition
plt.figure(figsize=(10, 6))
sns.countplot(y='Medical Condition', data=df,
              order=df['Medical Condition'].value_counts().index)
plt.title('Distribusi Data Berdasarkan Kondisi Medis')
plt.xlabel('Jumlah Pasien')
plt.ylabel('Kondisi Medis')
plt.show()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
"""

# 1. Menghapus atau Menangani Data Kosong (Missing Values)
print("Cek Missing Values Awal:")
print(df.isnull().sum())

# Asumsi: Hanya ada sedikit missing values, hapus (sesuaikan dengan konteks dataset)
df = df.dropna()

# 2. Menghapus Data Duplikat
print(f"\nJumlah Duplikasi Awal: {df.duplicated().sum()}")
df = df.drop_duplicates()
print(f"Jumlah Duplikasi Setelah Dihapus: {df.duplicated().sum()}")

# 3. Encoding Data Kategorikal
# Identifikasi kolom kategorikal yang perlu di-encode
categorical_cols = df.select_dtypes(include='object').columns.tolist()
# Kita akan mengecualikan kolom 'Name' dan 'Patient ID' jika ada, karena tidak relevan untuk training
cols_to_encode = [col for col in categorical_cols if col not in ['Name', 'Patient ID']]

encoder = LabelEncoder()
for col in cols_to_encode:
    # Mengatasi kolom yang mungkin muncul saat cleaning data
    if col in df.columns:
        df[col + '_Encoded'] = encoder.fit_transform(df[col])
        # Opsional: hapus kolom asli jika sudah di-encode
        df = df.drop(columns=[col])

print("\n5 Baris Pertama Setelah Encoding:")
print(df.head())

# 4. Normalisasi atau Standarisasi Fitur (Untuk kolom numerik target/penting) ---
# Misalnya, kita standarisasi 'Billing Amount'
scaler = StandardScaler()
# Pastikan kolom Billing Amount ada dan tipenya numerik (diasumsikan 'float'/'int')
if 'Billing Amount' in df.columns:
    df['Billing_Amount_Scaled'] = scaler.fit_transform(df[['Billing Amount']])

# Tampilkan hasil akhir preprocessing (termasuk kolom baru)
print("\nStruktur Akhir Dataset:")
print(df.info())

# --- Simpan dataset hasil preprocessing ---
cleaned_file_path = 'healthcare_dataset_cleaned.csv'
df.to_csv(cleaned_file_path, index=False)
print(f"\nDataset telah berhasil disimpan sebagai '{cleaned_file_path}'")

!pip freeze requirements.txt

from google.colab import files

!pip freeze > requirements.txt
files.download('requirements.txt')

# automate_Nama-siswa.py (Contoh)

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
import os

def preprocess_data(raw_data_path):
    """
    Melakukan semua langkah preprocessing yang didefinisikan di notebook eksperimen.
    """
    df = pd.read_csv(raw_data_path)

    # 1. Penanganan Missing Values dan Duplikat
    df = df.dropna()
    df = df.drop_duplicates()

    # 2. Encoding Kategorikal (contoh menggunakan LabelEncoder)
    cols_to_encode = ['Gender', 'Blood Type', 'Medical Condition'] # Sesuaikan dengan dataset Anda
    encoder = LabelEncoder()
    for col in cols_to_encode:
        if col in df.columns:
            df[col + '_Encoded'] = encoder.fit_transform(df[col])
            df = df.drop(columns=[col])

    # 3. Scaling Numerik (contoh: Billing Amount)
    scaler = StandardScaler()
    if 'Billing Amount' in df.columns:
        df['Billing_Amount_Scaled'] = scaler.fit_transform(df[['Billing Amount']])
        df = df.drop(columns=['Billing Amount']) # Hapus kolom asli

    # Opsional: Hapus kolom non-fitur (Name, Patient ID, dll.)
    df = df.drop(columns=['Name', 'Patient ID'], errors='ignore')

    return df

if __name__ == '__main__':
    # Contoh penggunaan dan penyimpanan

    # Pastikan direktori ada sebelum menyimpan file
    output_dir = 'preprocessing/namadataset_preprocessing'
    os.makedirs(output_dir, exist_ok=True)

    cleaned_df = preprocess_data('healthcare_dataset.csv')
    cleaned_df.to_csv(os.path.join(output_dir, 'healthcare_cleaned.csv'), index=False)
    print("Preprocessing otomatis selesai. Data disimpan.")